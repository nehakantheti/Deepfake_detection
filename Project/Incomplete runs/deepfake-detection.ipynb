{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5380830,"sourceType":"datasetVersion","datasetId":3120670}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"rm -rf /kaggle/working/*","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install moviepy\n!pip install autokeras\n!pip install --upgrade keras_nlp\n# Fix for the plotting in Kaggle \n!pip install pydot\n!apt-get install -y graphviz\n!pip install graphviz","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport glob\nimport cv2\nimport shutil\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.layers import *\nfrom keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical, plot_model\nimport autokeras as ak\nfrom tensorflow.keras.models import save_model\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nimport librosa\nfrom sklearn.model_selection import train_test_split\nimport h5py","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting up paths\nsrc_dir_real = \"/kaggle/input/celeb-df-v2/Celeb-real\"\nsrc_dir_fake = \"/kaggle/input/celeb-df-v2/Celeb-synthesis\"\nsrc_dir_yt = \"/kaggle/input/celeb-df-v2/YouTube-real\"\ndest_dir_real = \"/kaggle/working/dataset/real\"\ndest_dir_fake = \"/kaggle/working/dataset/fake\"\ndest_dataset_root = \"/kaggle/working/dataset\"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating destination directories - if ignored, throws error while saving files\nos.makedirs(dest_dir_real, exist_ok=True)\nos.makedirs(dest_dir_fake, exist_ok=True)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_real = os.listdir(src_dir_real)\nprint(len(list_real))\nlist_fake = os.listdir(src_dir_fake)\nprint(len(list_fake))\nlist_yt = os.listdir(src_dir_yt)\nprint(len(list_yt))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of real videos in dataset\n\n# Copying real files path to from source to destination\nfor filename in list_real:\n    src_file = os.path.join(src_dir_real, filename)\n    dest_file = os.path.join(dest_dir_real, filename)\n    shutil.copy(src_file, dest_file)\n\nfor filename in list_yt:\n    src_file = os.path.join(src_dir_yt, filename)\n    dest_file = os.path.join(dest_dir_real, filename)\n    shutil.copy(src_file, dest_file)\n\n# Copying fake files path to from source to destination\nfor filename in list_fake:\n    src_file = os.path.join(src_dir_fake, filename)\n    dest_file = os.path.join(dest_dir_fake, filename)\n    shutil.copy(src_file, dest_file)\n\nprint(\"Files copied successfully!\")\nprint(len(os.listdir(dest_dir_real)), len(os.listdir(dest_dir_fake)))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Why extract audio?\n\nDeepfake videos often exhibit inconsistencies not just in visual cues but also in audio. By analyzing the audio component, the model can detect anomalies such as unnatural speech patterns, mismatches between lip movements and speech, or synthetic voice artifacts. Incorporating audio analysis enhances the model's ability to detect deepfakes more accurately","metadata":{"editable":false}},{"cell_type":"code","source":"def extract_audio_files():\n    real_video_files = glob.glob(os.path.join(\"/kaggle/working\", dataset_root, \"real\", \"*.mp4\"))\n    fake_video_files = glob.glob(os.path.join(\"/kaggle/working\", dataset_root, \"fake\", \"*.mp4\"))\n    all_video_files = real_video_files + fake_video_files\n    for video_file in all_video_files:\n\n        # Load the video file using moviepy to access its frames, duration, and audio\n        # This will be used for extracting audio or performing video edits\n        video_clip = VideoFileClip(video_file)\n\n        # Load the video file and extract its audio component for further processing\n        audio_clip = video_clip.audio\n        output_audio_path = os.path.splitext(video_file)[0] + \".wav\"\n        audio_clip.write_audiofile(output_audio_path, codec+'pcm_s16le')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loading and Extracting Video frames**","metadata":{"editable":false}},{"cell_type":"code","source":"def extract_video_frames(video_path, image_size, num_frames = 10):\n    # Loading video file\n    video_capture = cv2.VideoCapture(video_path)\n    # Get the total number of frames in the video\n    frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # print(f\"Frame Count : {frame_count}\")\n    \n    frames = []\n\n    # Loop through the video and extract up to 'num_frames' frames\n    for frame_num in range(min(frame_count, num_frames)):\n        ret, frame = video_capture.read()  # Read the next frame\n        if not ret:   # If frame not read successfully, exit loop\n            break\n\n        # Converting frames from BGR to RGB\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # Resizing images to parameter image_size\n        frame = cv2.resize(frame, image_size)\n        frames.append(frame)\n    # Returning processed frames\n    return frames","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_root = \"/kaggle/working/dataset\"\n\nreal_video_paths = glob.glob(os.path.join(dataset_root, \"real\", \"*.mp4\"))\nfake_video_paths = glob.glob(os.path.join(dataset_root, \"fake\", \"*.mp4\"))\n\n# Frame image size\nimage_size = (64, 64)\nnum_samples = 100\n\nprint(len(real_video_paths), len(fake_video_paths))\n\n# Extracting images(X = features) from video clips\n#X_images_real = [extract_video_frames(video_p0ath, image_size) for video_path in real_video_paths]\n#X_images_fake = [extract_video_frames(video_path, image_size) for video_path in fake_video_paths]\ndef extract_video_frames_safe(video_path, image_size, num_frames=10):\n    frames = extract_video_frames(video_path, image_size, num_frames)\n    return frames if len(frames) == num_frames else None\n\nX_images = []\nY_labels = []\n\nfor video_path in real_video_paths:\n    frames = extract_video_frames_safe(video_path, image_size)\n    if frames is not None:\n        X_images.append(frames)\n        Y_labels.append(0)  # Real → 0\n\nfor video_path in fake_video_paths:\n    frames = extract_video_frames_safe(video_path, image_size)\n    if frames is not None:\n        X_images.append(frames)\n        Y_labels.append(1)  # Fake → 1\n\nX_images = np.array(X_images)\nY_labels = np.array(Y_labels)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Valid videos:\", len(X_images))\nprint(\"Valid labels:\", len(Y_labels))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_images.shape, Y_labels.shape)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using random seed as 42\nX_images_train, X_images_test, Y_labels_train, Y_labels_test = train_test_split(\n    X_images, Y_labels, test_size = 0.2, random_state = 42\n)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_images_train shape:\", X_images_train.shape)\nprint(\"X_images_test shape:\", X_images_test.shape)\nprint(\"Y_train shape:\", Y_labels_train.shape)\nprint(\"Y_test shape:\", Y_labels_test.shape)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the data in a HDF5 file\nh5_path = '/kaggle/working/dataset/data.h5'\n\nif os.path.exists(h5_path):\n    os.remove(h5_path)\n    print(\"Deleted data.h5\")\nelse:\n    print(\"data.h5 not found\")\n\n# Save the data in a HDF5 file for faster processing and beter RAM utilization\nh5_file = h5py.File('/kaggle/working/dataset/data.h5', 'w')\n\nh5_file.create_dataset('X_images_train', data=X_images_train)\nh5_file.create_dataset('X_images_test', data=X_images_test)\nh5_file.create_dataset('y_labels_train', data=Y_labels_train)\nh5_file.create_dataset('y_labels_test', data=Y_labels_test)\n\n# Close the HDF5 file\nh5_file.close()\nprint(\"Successfully loaded data into file!\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flush out the heavily loaded variables from memory\ndel X_images_train\ndel X_images_test\ndel Y_labels_train\ndel Y_labels_test","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"latent_dim = 10\nimage_shape = (64, 64, 3)\noutput_shape = (64, 64, 3)\n\n# Build the Generator model\ndef build_generator(latent_dim, output_shape):\n    generator_input = Input(shape=(latent_dim,))\n    x = Dense(64)(generator_input)\n    x = LeakyReLU()(x)\n    x = Dense(128)(x)\n    x = LeakyReLU()(x)\n\n    # Calculate the number of units in the Dense layer for the generator output\n    num_units = output_shape[0] * output_shape[1] * output_shape[2]\n\n    # Generator output with a Dense layer and reshape\n    generated_data = Dense(num_units, activation='tanh')(x)\n    generated_data = Reshape(output_shape)(generated_data)\n\n    generator = Model(generator_input, generated_data)\n    return generator\n\n# Build the Discriminator model\ndef build_discriminator(input_shape):\n    discriminator_input = Input(shape=input_shape)\n    x = Dense(128)(discriminator_input)\n    x = LeakyReLU()(x)\n    x = Dense(64)(x)\n    x = LeakyReLU()(x)\n    validity = Dense(1, activation='sigmoid')(x)\n    discriminator = Model(discriminator_input, validity)\n    return discriminator\n\n# Build the GAN model\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    gan_input = Input(shape=(latent_dim,))\n    generated_data = generator(gan_input)\n    gan_output = discriminator(generated_data)\n    gan = Model(gan_input, gan_output)\n    return gan\n\ngenerator = build_generator(latent_dim, output_shape)\ndiscriminator = build_discriminator(output_shape)\ngan = build_gan(generator, discriminator)\n\n# Print the model summaries\ngenerator.summary()\ndiscriminator.summary()\ngan.summary()\n\n# Plot the model architectures\nplot_model(generator, show_shapes=True, to_file=\"generator.png\")\nplot_model(discriminator, show_shapes=True, to_file=\"discriminator.png\")\nplot_model(gan, show_shapes=True, to_file=\"gan.png\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def multimodal_fusion(images, num_modalities):\n    num_samples, num_frames, height, width, channels = images.shape\n\n    # Create additional modalities by converting images to grayscale\n    additional_modalities = []\n    for i in range(1, num_modalities):\n        modified_images = []\n        for frame in range(num_frames):\n            grayscale_frame = np.mean(images[:, frame], axis=-1, keepdims=True)\n            modified_images.append(grayscale_frame)\n        additional_modalities.append(np.array(modified_images))\n\n    # Tile the additional modalities to match the number of frames\n    tiled_modalities = [modalities[:, np.newaxis] for modalities in additional_modalities]\n    tiled_modalities = [np.tile(modalities, (1, num_frames, 1, 1, 1)) for modalities in tiled_modalities]\n\n    # Concatenate the original images and additional modalities along the channel axis\n    fused_data = np.concatenate((images, *tiled_modalities), axis=-1)\n    return fused_data","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nbatch_size = 40\nnum_modalities = 1 #Add GrayScale with RGB modality\n\n# Load the data from the HDF5 file\nh5_file = h5py.File('/kaggle/working/dataset/data.h5', 'r')\nnum_samples = len(h5_file['X_images_train'])\ny_train = h5_file['y_labels_train'][:]\n\nmultimodal_data = []\n\nfor batch_start in range(0, num_samples, batch_size):\n    batch_end = min(batch_start + batch_size, num_samples)\n    batch_X_images_train = h5_file['X_images_train'][batch_start:batch_end]\n    #batch_X_images_test = h5_file['X_images_test'][batch_start:batch_end]\n\n    # Perform multimodal fusion on the current batch\n    fused_batch_X_train = multimodal_fusion(batch_X_images_train, num_modalities)\n    #fused_batch_X_test = multimodal_fusion(batch_X_images_test, num_modalities)\n    multimodal_data.append(fused_batch_X_train)\n    print(\"Fused batch X_train shape:\", fused_batch_X_train.shape)\n    #print(\"Fused batch X_test shape:\", fused_batch_X_test.shape)\n\nh5_file.close()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import class_weight\nimport numpy as np\n\n# weight sampling for imbalanced classes\nclass_weights = class_weight.compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(reshaped_y_train),\n    y=reshaped_y_train\n)\n\n# Convert to dictionary for Keras\nclass_weights = dict(enumerate(class_weights))\nprint(class_weights)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import autokeras as ak\nfrom sklearn.model_selection import train_test_split\n# Stack the multimodal data\nfused_X_train = np.vstack(multimodal_data)\n\n# Get the dimensions from the shape of fused_X_train\nnum_samples, num_frames, height, width, channels = fused_X_train.shape\n\n# Reshape the data to match AutoKeras expected format\nreshaped_X_train = fused_X_train.reshape((-1, height, width, channels))\n\n# Reshape again to treat each frame as a separate sample\nreshaped_X_train = reshaped_X_train.reshape((-1, height, width, channels))\n\n# Reshape y_train to match the new number of samples\nreshaped_y_train = np.repeat(y_train, num_frames, axis=0)\n\nprint(\"reshaped_X_train shape:\", reshaped_X_train.shape)\nprint(\"reshaped_y_train shape:\", reshaped_y_train.shape)\n\nX_train, X_val, y_train, y_val = train_test_split(reshaped_X_train, reshaped_y_train, test_size=0.2, random_state=42)\n\n# Create the image-based AutoKeras model\nsearch = ak.ImageClassifier(max_trials=2)\n\n# Train the NAS model\nhistory = search.fit(\n    reshaped_X_train, \n    reshaped_y_train, \n    epochs=4, \n    validation_data=(X_val, y_val),\n    class_weight = class_weights)\nprint(history.history.keys())\n\n# Export the best model architecture\nbest_model = search.export_model()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\nsave_model(best_model, '/kaggle/working/best_model.h5')\n\n# Print the model summary\nbest_model.summary()\n\n# Plot the model architecture\nplot_model(best_model, show_shapes=True, to_file=\"best_model.png\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the best model\nbest_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train the model\nbest_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = best_model.evaluate(X_val, y_val)\nprint(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Value')\nplt.legend(loc='lower right')\nplt.title('Training History')\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have a trained model and test data\ny_pred = model.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)  # For multi-class classification","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_labels, target_names=['Real', 'Fake']))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred_labels)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# For binary classification, ensure y_test and y_pred are in the correct format\nroc_auc = roc_auc_score(y_test, y_pred[:, 1])\nprint(f\"ROC AUC Score: {roc_auc}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}